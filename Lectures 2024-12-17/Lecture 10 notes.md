## TĘSINYS iš 9 Paskaitos

Vaizdo įrašas: https://youtu.be/8rcOvkJdeVw
Paskaitos skaidrės: https://github.com/jurjan/AI_Project_Management/blob/main/Lectures%202024-12-17/Paskaita9.pdf

Trumpas paskaitos aprašas:

Iki šiol daugiausia aptarėme, kaip priešiškos atakos veikia prediktyviąją AI ir diskriminacinius modelius. Šie modeliai yra sukurti tam, kad atskirtų skirtingus duomenų tipus. Jie puikiai geba analizuoti, klasifikuoti ir prognozuoti specifinius rezultatus, remdamiesi įvesties duomenimis. Šioje dalyje nagrinėsime generative AI – kitokį AI tipą, kuris generuoja visiškai naujus duomenis, tokius kaip tekstai ar vaizdai, remdamasis savo mokymo duomenimis. Abu AI tipai remiasi tais pačiais pagrindais.

GAN (generatyvūs priešingų tinklų) modeliai naudoja du neuroninius tinklus naujų duomenų pavyzdžiams generuoti, konkuruodami siekiant optimizuoti sugeneruotų duomenų tikslumą.

Auto-encoders are a class of encoder/decoder models. Basic auto-encoders are unsupervised neural networks compressing input data into a lower-dimensional representation (encoding) and then reconstructing the output (decoding) to match the input as closely as possible. Although, strictly speaking, they are not generative models, they can be used in generative settings such as deepfake generation. VAEs, on the other hand, are designed explicitly as generative models. Instead of decoding the learned feature, they generate new data from the latent space, which has been used to encode input as a distribution.
Recurrent neural networks (RNNs): These were introduced in Chapter 1 as networks that store previous steps, making them ideal for processing sequential data in predictive AI. They are equally effective for sequence generation tasks in generative AI, especially their advanced long short-term memory (LSTM) variants. They are widely used in applications such as music composition, text generation, and predictive typing. A popular use of RNNs has been sequence-to-sequence (seq2seq) models for neural machine translation. RNN-based seq2seq consists of two RNNs (an encoder and a decoder). A challenge for seq2seq models has been the relatively limited memory of a single fixed-size vector in RNNs. LSTMs increase the model’s memory, and a seq2seq variation called seq2seq with attention has been introduced to address the problem. This configuration adds decoding outputs to the next encoding step and focuses on relevant data points. 
Transformer models: Despite the attention optimizations, RNNs still face challenges with vanishing gradients and an architecture that is not designed to process long sequences and hierarchies in parallel. As a result, the focus of seq2seq has moved to transformer models, which eliminate the bottleneck of a single vector. These models were introduced by Google in 2017 and still follow the encoder-decoder pattern, but they use stacks of encoders and decoders and rely purely on self-attention and feedforward layers. This eliminates the RNN sequence processing and allows input points to reference each other simultaneously.
Energy-based models (EBMs) are generative models that borrow statistical mechanics from physics to learn the distribution of a dataset and then generate completely new samples. Research published by OpenAI in 2019 demonstrated its successful use to create new samples for ImageNet32x32, ImageNet128x128, and CIFAR-10, as well as robotic hand trajectories on par with those of GANs and adversarial robustness without any adversarial training. You can find the research at https://arxiv.org/abs/1903.08689. Restricted Boltzmann machines (RBMs) are well-known EBMs with two layers: a visible layer that represents the observed data and a hidden layer that captures latent features



, GANs are as effective as other forms of data, such as text. Their unique ability to generalize a dataset distribution and produce data variances can aid offensive testers and red teams in brute-forcing attacks.
PassGAN is an example of using a GAN to improve password brute-forcing attacks. Usually, tools such as John the Ripper (JtR) and HashCat are state-of-the-art brute-forcing tools that allow attackers and red teams to crack a password by trying millions of hashes a second using combinatorial generation or dictionary attacks and replay curated datasets based on previous password data breaches (wordlists). They can also allow attackers to customize candidate passwords using rules that include concatenation or wildcards and leet speak, which involves replacing certain characters with digits or symbols – for example, p@s$wo0rd. These tools have been effective, with a success rate of over 90% against online leaked services. However, their approach requires human intuition in choosing a dictionary and creating rules, which can make this a laborious exercise.
PassGAN, on the other hand, utilizes generative adversarial training to learn the distribution of passwords from leaks from 90% of the popular password dataset RockYou with 10% of the dataset used for training. The dataset contains more than 32M passwords from various leaks, and only passwords of 10 characters or more were used for training. The model was further evaluated using the LinkedIn leak dataset, and the results were compared with JtR, HashCat, and an RNN-based password-guessing model called FLA. The results highlighted that GANs can produce good candidate passwords based on the underlying distribution and passwords that the custom rules would not have created


