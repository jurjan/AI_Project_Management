# Privacy-Preserving AI: Privacy-preserving machine learning techniques. Federated learning and differential privacy

Paskaitos įrašas: https://youtu.be/Yyw7whyFz20

Paskaitos skaidrės:https://github.com/jurjan/AI_Project_Management/blob/main/Lectures%202025-01-06/Lecture%2012.pdf

Medžiaga parengta remiantis:

* https://doi.org/10.1016/j.diggeo.2024.100104
* J. Morris Chang, Di Zhuang, G. Dumindu Samaraweera - Privacy-Preserving Machine Learning
* Shui Yu, Lei Cui - Security and Privacy in Federated Learning
* https://arxiv.org/abs/2102.04704
* Klaus Haller. Managing AI in the Enterprise. Succeeding with AI Projects and MLOps to Build Sustainable AI Organizations

## Federated learning related material:

* https://www.researchgate.net/publication/347178320_Threats_to_Federated_Learning

## DP/LDP related material:
* https://onlinelibrary.wiley.com/doi/10.1155/2020/8829523
* https://research.google/blog/making-ml-models-differentially-private-best-practices-and-open-challenges/
* https://diffprivlib.readthedocs.io/en/latest/modules/mechanisms.html
* https://dl.acm.org/doi/10.1145/2857705.2857708
* https://rbcborealis.com/research-blogs/tutorial-12-differential-privacy-i-introduction/

## Trumpas paskaitos aprašas 

Dalis medžiagos yra skaidrėse, bet nebuvo spėta išdėstyti paskaitos metu, todėl pateikta čia raštu.

Duomenimis pagrįstas dirbtinis intelektas (DI) reiškia dirbtinio intelekto sistemas ir taikymus, kurie stipriai remiasi duomenimis, kad mokytųsi, atliktų prognozes ar vykdytų užduotis. Šis terminas pabrėžia duomenų vaidmenį kaip pagrindą DI modelių mokymui ir tobulinimui. Vietoje to, kad būtų aiškiai užprogramuotos konkrečioms užduotims, šios AI sistemos gauna įžvalgas, modelius ir sprendimus iš didelio kiekio struktūruotų ir nestruktūruotų duomenų.

* Prisitaikymas (Adaptability) - Dirbtinio intelekto (DI) sistemos gali prisitaikyti prie naujų duomenų ir besikeičiančios aplinkos, todėl jos yra lanksčios įvairioms taikymo sritims.
* Duomenys kaip pagrindinis įvestis (Data as the Core Input) - Duomenimis paremto DI veiksmingumas priklauso nuo naudojamų duomenų kokybės, kiekio ir įvairovės.
* Iteracinis procesas (Iterative Process) - Apima nuolatinį duomenų rinkimą, valymą, žymėjimą ir jų pateikimą modeliams, siekiant juos tobulinti ir gerinti našumą.
Duomenimis grįstas DI (pavyzdžiai):
* Prognozavimo analizė - DI prognozuoja akcijų kainas ar klientų nutekėjimą remiantis istoriniais duomenimis.
* Natūralios kalbos apdorojimas (NLP) - Sistemos, tokios kaip pokalbių robotai ar vertimo paslaugos, mokomos naudojant tekstinius duomenis.
* Rekomendacijų sistemos - El. prekybos platformos siūlo produktus pagal vartotojo naršymo duomenis.
* Vaizdų atpažinimas - DI aptinka objektus ar diagnozuoja ligas pagal vaizdų duomenų rinkinius.
Pagrindinės sudedamosios dalys:
* Duomenų rinkimas - Aktualių duomenų rinkinių rinkimas iš jutiklių, žurnalų, vartotojų sąveikų ar išorinių šaltinių.
* Duomenų paruošimas - Duomenų valymas, transformavimas ir struktūravimas, siekiant užtikrinti jų tinkamumą DI modeliams.
* Modelio mokymas - Mašininio mokymosi algoritmų, tokių kaip neuroniniai tinklai, sprendimų medžiai (decission trees) ar klasterizavimo modeliai (clustering models), naudojimas.
* Vertinimas - Modelio veikimo matavimas naudojant tokius rodiklius kaip tikslumas, precizija ar atgaminimas.
* Diegimas ir stebėsena - Apmokyto modelio diegimas realiose programose ir jo veikimo stebėjimas.

Duomenų rinkimas vyksta mūsų mobiliuosiuose įrenginiuose ir kompiuteriuose, gatvėse, netgi mūsų biuruose ir namuose, o surinkti duomenys naudojami įvairiose mašininio mokymosi (ML) taikymo srityse, tokiose kaip rinkodara, draudimas, finansinės paslaugos, mobilumas, socialiniai tinklai ir sveikatos priežiūra. Yra net specifinis terminas Machine Learning  as a Service (LMaaS).  LMaaS - cloud-based ML and computing resources are bundled together to provide efficient analytical platforms (such as Microsoft Azure Machine Learning Studio, AWS Machine Learning, and Google Cloud Machine Learning Engine). Yra Vis daugiau debesijoje veikiančių, duomenimis pagrįstų ML taikymų, kurie kuriami skirtingų paslaugų teikėjų (gali būti klasifikuojami kaip duomenų naudotojai, tokie kaip „Facebook“, „LinkedIn“ ir „Google“). Dauguma tokio tipo taikymų naudoja didžiulius duomenų kiekius, surinktus iš kiekvieno individo (duomenų savininko), siekdami pasiūlyti naudotojams vertingas paslaugas. Šios paslaugos dažnai suteikia vartotojams komercinį ar politinį pranašumą, palengvindamos įvairias rekomendacijas, veiklos atpažinimą, sveikatos stebėseną, tikslinę reklamą ar net rinkimų prognozes. Tačiau kita vertus, tie patys duomenys gali būti panaudoti siekiant išvesti jautrią (privačią) informaciją, kas galėtų kelti grėsmę asmenų privatumui. Todėl natūralu, jog susiduriama su saugumo pažeidimais ir įvairiomis su tuo susijusiomis rizikomis, apie ką jau kalbėjome praeitose paskaitose (Privacy complications in the AI era). 

Kai asmeninė informacija naudojama netinkamam ar nenumatytam tikslui, ja gali būti manipuliuojama siekiant įgyti konkurencinį pranašumą. Kai didžiuliai kiekiai asmeninių duomenų yra derinami su mašininio mokymosi algoritmais, niekas negali numatyti, kokius naujus rezultatus jie gali generuoti ar kiek privačios informacijos tie rezultatai gali atskleisti. Todėl kuriant mašininio mokymosi algoritmus tam tikrai taikymo sričiai, būtina užtikrinti privatumo apsaugą. Pirma, tai užtikrina, kad kitos šalys (duomenų naudotojai) negalėtų pasinaudoti asmeniniais duomenimis savo naudai. Antra, kiekvienas turi dalykų, kurių nenori atskleisti kitiems.
Tradiciškai duomenų saugumo ir privatumo reikalavimus nustatydavo duomenų savininkai (pvz., organizacijos), siekdami apsaugoti savo siūlomų produktų ir paslaugų konkurencinį pranašumą. Tačiau didžiųjų duomenų eroje duomenys tapo vertingiausiu turtu skaitmeninėje ekonomikoje, o vyriausybės įvedė daugybę privatumo reguliavimų, siekdamos užkirsti kelią jautrios informacijos naudojimui ne pagal jos numatytą tikslą. Tarp dažniausiai organizacijų laikomų privatumo standartų yra HIPAA (1996 m. Sveikatos draudimo perkėlimo ir atskaitomybės aktas), PCI DSS (Mokėjimo kortelių pramonės duomenų saugumo standartas), FERPA (Šeimos švietimo teisių ir privatumo aktas) bei Europos Sąjungos BDAR (Bendrasis duomenų apsaugos reglamentas).
Pavyzdžiui, nepriklausomai nuo veiklos masto, dauguma sveikatos priežiūros teikėjų elektroniniu būdu perduoda sveikatos informaciją, pvz., apie pretenzijas, vaistų įrašus, naudos tinkamumo užklausas ar siuntimų autorizacijos užklausas. Tačiau HIPAA reikalavimai įpareigoja šiuos sveikatos priežiūros teikėjus apsaugoti jautrią pacientų sveikatos informaciją nuo atskleidimo be paciento sutikimo ar žinios. ML modelio gebėjimas tiksliai prognozuoti galutinį rezultatą rodo, kaip gerai modelis generalizuoja naujus arba pirmą kartą pateiktus duomenis. Šis tikslumas paprastai matuojamas empiriškai ir priklauso nuo tokių veiksnių kaip mokymo pavyzdžių skaičius (training data), modelio kūrimo algoritmas, mokymo pavyzdžių kokybė ir ML algoritmo hiperparametrų pasirinkimas.
Taip pat labai svarbu apdoroti neapdorotus (raw) duomenis prieš juos perduodant ML modeliams. Nepaisant to, ar duomenys yra pažymėti, ar ne, ir ar duomenų apdorojimas buvo atliktas, ML modeliai iš esmės yra labai sudėtinga statistika, paremta mokymo duomenų rinkiniu. Kai siekiame išsaugoti privatumą, norime užkirsti kelią, kad algoritmai išmoktų kažkokius jautrius atributus. Naudingumas ir privatumas yra priešinguose spektro galuose. Griežtinant privatumo apsaugą, gali nukentėti naudingumo efektyvumas. Pagrindinis iššūkis yra subalansuoti privatumą ir veikimą ML taikymuose taip, kad galėtume geriau išnaudoti duomenis užtikrindami asmenų privatumą. Dėl reguliavimo ir specifinių taikymo reikalavimų negalime sumažinti privatumo apsaugos vien tam, kad padidintume taikymo naudingumą. ML taikymai yra linkę į įvairias privatumo ir saugumo atakas. 
Rekonstrukcijos atakos kelia dar vieną galimą grėsmę: užpuolikas gali atkurti duomenis net neturėdamas prieigos prie viso neapdorotų duomenų rinkinio serveryje. Tokiu atveju priešininkas įgyja pranašumą, turėdamas išorinę žinių apie požymių vektorius (duomenis, naudojamus ML modeliui kurti).
Nors kai kurie ML modeliai saugo aiškius požymių vektorius, kiti ML algoritmai nelaiko požymių vektorių modelio viduje. Tokiais atvejais priešininko žinios yra ribotos, tačiau jie vis tiek gali turėti prieigą prie ML modelio, kaip aptarta „baltosios dėžės“ prieigos scenarijuje. Kitu „juodosios dėžės“ prieigos scenarijumi priešininkas neturi tiesioginės prieigos prie ML modelio: jie gali stebėti gaunamas užklausas ML modeliui, kai vartotojas pateikia naujus testavimo pavyzdžius, ir atsakymus, kuriuos generuoja modelis. Modelio inversijos atakų metu priešininkas išnaudoja ML modelio generuojamus atsakymus taip, kad jie primena pradinius požymių vektorius, naudotus ML modeliui kurti.
Narystės išvedimo atakos (membership inference attacks) bando nustatyti, ar tam tikras pavyzdys priklausė pradiniam mokymo duomenų rinkiniui, remiantis ML modelio išvestimi. Narystės išvedimo atakos idėja yra tokia: turint ML modelį, pavyzdį ir žinių apie taikomąją sritį, priešininkas gali nustatyti, ar tas pavyzdys buvo dalis mokymo duomenų rinkinio, naudoto ML modeliui kurti.
Koreliacinės atakos (correlation attacks) - Galutinis koreliacinės atakos tikslas yra nustatyti koreliaciją tarp dviejų ar daugiau duomenų laukų duomenų bazėje arba duomenų bazių instancijų rinkinyje.
Identifikavimo atakos (identification attacks) - identifikavimo ataka siekia identifikuoti tikslinį asmenį, susiejant įrašus duomenų bazės instancijoje. Šios atakos tikslas – atskleisti daugiau asmeninės informacijos apie konkretų asmenį identifikavimo tikslais.

Daugelis privatumo stiprinimo technikų yra orientuotos į tai, kad keli duomenų teikėjai galėtų bendradarbiauti treniruojant ML modelius, nepateikiant privačių duomenų jų pradiniu pavidalu. Toks bendradarbiavimas gali būti vykdomas naudojant kriptografinius metodus (pvz., saugią daugiapusių skaičiavimų (secure multiparty computation) techniką) arba diferencinio privatumo duomenų paskelbimą (trikdymo technikas). 
* Diferencinis privatumas (DP) yra perspektyvus sprendimas, skirtas užtikrinti duomenų privatumo apsaugą. Siekiama apsaugoti asmens jautrią informaciją nuo bet kokių išvedimo atakų, nukreiptų į asmens statistiką ar agreguotus duomenis.
* Diferencinis privatumas (DP) grindžiamas idėja, kad statistika ar agreguoti duomenys (įskaitant ML modelius) neturėtų atskleisti, ar asmuo yra pradiniame duomenų rinkinyje (ML modelių mokymo duomenyse). Pavyzdžiui, turint du identiškus duomenų rinkinius – vieną, kuriame yra asmens informacija, ir kitą, kuriame tokios informacijos nėra – DP užtikrina, kad tam tikros statistikos ar agreguotų reikšmių generavimo tikimybė būtų beveik vienoda, neatsižvelgiant į tai, kuris iš šių duomenų rinkinių yra naudojamas.
Kad būtų aiškiau, įsivaizduokime patikimą duomenų tvarkytoją, kuris renka duomenis iš kelių duomenų savininkų ir atlieka tam tikrą skaičiavimą, pavyzdžiui, apskaičiuoja vidutinę reikšmę arba suranda maksimalią ar minimalią reikšmę. Siekiant užtikrinti, kad niekas negalėtų patikimai išvesti jokio individualaus pavyzdžio iš skaičiavimo rezultato, DP reikalauja, kad tvarkytojas pridėtų atsitiktinį triukšmą prie rezultato. Tai garantuoja, kad paskelbti duomenys nesikeis, jei bus pakeistas bet kuris pradinio duomenų rinkinio pavyzdys. Kadangi nė vienas pavyzdys negali reikšmingai paveikti skirstinio, priešininkai negali užtikrintai išvesti informacijos, susijusios su bet kuriuo individualiu pavyzdžiu. Taigi mechanizmas laikomas DP užtikrinančiu, jei duomenų skaičiavimo rezultatai yra atsparūs bet kokiems individualių pavyzdžių pokyčiams.
Kai duomenų teikėjai neturi pakankamai informacijos ML modeliui treniruoti, naudingiau naudoti metodus, paremtus lokaliuoju diferenciniu privatumu (LDP). Pavyzdžiui, kelios vėžio tyrimų institucijos nori sukurti ML modelį odos pažeidimams diagnozuoti, tačiau nė viena šalis neturi pakankamai duomenų modeliui treniruoti. LDP yra vienas iš sprendimų, kurį jos gali naudoti bendradarbiaujant treniruoti ML modelį, nepažeidžiant individualaus privatumo.
* Lokalaus diferencinio privatumo (LDP) metoduose asmenys siunčia savo duomenis duomenų agregatoriui po to, kai jie privatizuojami naudojant trikdymą. Duomenų agregatorius surenka visus trikdytus duomenis ir apskaičiuoja statistiką, pavyzdžiui, kiekvienos reikšmės dažnį populiacijoje. Palyginti su diferenciniu privatumu (DP), LDP perkelia trikdymo procesą iš centrinės vietos į vietinį duomenų savininką. Jis taikomas scenarijams, kai nėra patikimos trečiosios šalies, o nepatikimas duomenų tvarkytojas turi rinkti duomenis iš duomenų savininkų ir atlikti tam tikrus skaičiavimus. Tokiu atveju duomenų savininkai vis dar nori prisidėti savo duomenimis, tačiau būtina užtikrinti šių duomenų privatumą.
* https://www.mdpi.com/2227-7390/11/7/1718

* Kai nėra iš anksto nustatyto algoritmo reikalingai operacijai atlikti, duomenų naudotojai gali paprašyti duomenų, kad galėtų juos naudoti vietoje. Šiuo tikslu praeityje buvo pasiūlytos įvairios privatumo apsaugą užtikrinančios duomenų dalijimosi technikos, tokios kaip k-anonimiškumas (k-anonymity), l-įvairovė (l-diversity), t-artumas (t-closeness) ir duomenų trikdymas (data perturbation).
* Sintetiniai duomenys yra dirbtinai generuojami, o ne gaunami iš realių. Jie paprastai kuriami algoritmiškai ir dažnai naudojami kaip pakaitalas ML modelių mokymui ir testavimui. Dalijimasis sintetiniais duomenimis, kurie yra tokio paties formato ir išlaiko tas pačias statistines charakteristikas kaip originalus duomenų rinkinys, suteikia duomenų naudotojams daug daugiau lankstumo ir minimaliai kelia privatumo problemų.

Tarp privatumo apsaugą užtikrinančių duomenų gavybos (PPDM) technikų dauguma remiasi duomenų modifikavimu arba dalies originalaus turinio pašalinimu siekiant apsaugoti privatumą. Toks duomenų „išvalymas“ ar transformavimas lemia kokybės sumažėjimą, kuris atspindi kompromisą tarp duomenų kokybės ir privatumo lygio. Nepaisant to, pagrindinė visų PPDM technikų idėja yra efektyviai išgauti duomenis, tuo pačiu išsaugant asmenų privatumą. Yra trys pagrindinės PPDM technikų klasės, kurios skirstomos pagal duomenų rinkimo, publikavimo ir apdorojimo etapus.
1. Pirmoji PPDM technikų klasė užtikrina privatumą duomenų rinkimo etape. Šios technikos dažniausiai naudoja įvairias atsitiktinimo metodikas rinkimo metu, generuodamos privatizuotas vertes, todėl originalios vertės niekada nėra saugomos. Dažniausiai naudojamas atsitiktinimo metodas – duomenų modifikavimas pridedant triukšmą, kuris turi žinomą pasiskirstymą. Naudojant duomenų gavybos algoritmus, galima atkurti pradinį duomenų pasiskirstymą, tačiau ne individualias vertes. Vieni iš labiausiai paplitusių šios kategorijos duomenų atsitiktinimo metodų yra adityvinio ir multiplikatyvinio triukšmo (aditive and multiplicative noise) metodai. https://www.sciencedirect.com/topics/computer-science/multiplicative-noise
2. Antroji PPDM technikų klasė susijusi su metodais, kai duomenys perduodami trečiosioms šalims (publikuojami) neskelbiant jautrios informacijos savininkų. Pašalinti atributus, kurie gali aiškiai identifikuoti asmenį, iš duomenų rinkinio nepakanka, nes vartotojai vis tiek gali būti identifikuoti derinant nejautrius atributus ar įrašus. PPDM technikos paprastai apima vieną ar kelias duomenų valymo operacijas, tokias kaip generalizavimas, slopinimas, anatomizacija ir trikdymas. Remiantis šiomis valymo operacijomis, galima pasiūlyti privatumo modelių rinkinį, kurie dabar plačiai naudojami įvairiose taikymo srityse siekiant užtikrinti privatumo apsaugą.
3. https://www.researchgate.net/publication/344753623_Data_mining_privacy_preserving_Research_agenda

Net ir turint tik netiesioginę prieigą prie pradinio duomenų rinkinio, duomenų gavybos algoritmų rezultatai gali atskleisti privačią informaciją apie pagrindinį duomenų rinkinį. Aktyvus priešininkas gali pasinaudoti šiais algoritmais ir atlikti užklausas, kad išvestų tam tikrą privačią informaciją. Dėl to buvo pasiūlytos įvairios technikos, siekiant apsaugoti duomenų gavybos algoritmų išvesties privatumą.

Asociacijų taisyklių slėpimo idėja yra išgauti tik nejautrias taisykles, užtikrinant, kad jokios jautrios taisyklės nebūtų atskleistos. Paprasčiausias metodas – trikdyti duomenų įrašus taip, kad būtų paslėptos visos jautrios, bet ne nejautrios taisyklės.

Klasifikatoriaus efektyvumo mažinimas. Kai kuriose taikymo srityse vartotojai gali pateikti užklausas pradiniam duomenų rinkiniui, tačiau jų užklausų funkcionalumas yra ribotas, pavyzdžiui, leidžiamos tik agregavimo užklausos (SUM, AVERAGE ir pan.). Vis dėlto priešininkas gali išvesti tam tikrą privačią informaciją, analizuodamas užklausų sekas ir jų atitinkamus rezultatus. Tokiose situacijose privatumo apsaugai dažnai naudojamas užklausų auditas, kuris arba trikdo užklausų rezultatus, arba atmeta vieną ar daugiau užklausų iš sekos. Tačiau šio metodo trūkumas yra tai, kad jo skaičiavimo sudėtingumas yra daug didesnis nei kitų metodų.
Kompresinis privatumas trikdo duomenis projektuodamas juos į mažesnio matmens hiperplokštumą, naudojant kompresijos ir matmenų mažinimo technikas. Dauguma šių transformavimo metodų yra nuostolingi.

Duomenų tvarkytojas prideda atsitiktinį triukšmą (naudodamas DP „sanitizatorių“) prie skaičiavimo rezultato, kad paskelbti rezultatai nepasikeistų, jei pagrindiniuose duomenyse pasikeistų kokio nors asmens informacija. Kadangi nė vieno asmens informacija negali reikšmingai paveikti skirstinio, priešininkai negali užtikrintai išvesti, kad kokia nors informacija atitinka konkretų asmenį.Viena iš pagrindinių techninių problemų diferenciniame privatume (DP) yra nustatyti atsitiktinio triukšmo kiekį, kuris turi būti pridėtas prie agreguotų duomenų prieš juos publikuojant. Atsitiktinis triukšmas negali būti generuojamas iš bet kokio atsitiktinio kintamojo (arbitrary random variable).

Mechanizmai:
* https://diffprivlib.readthedocs.io/en/latest/modules/mechanisms.html
* https://rbcborealis.com/research-blogs/tutorial-12-differential-privacy-i-introduction/

Įvesties trikdymo (input perturbation) atveju triukšmas yra tiesiogiai pridedamas prie įvesties duomenų (mokymo duomenų). Atlikus norimą neprivatų ML algoritmo skaičiavimą (ML mokymo procedūrą) su išvalytais duomenimis, gauta išvestis (ML modelis) bus diferencinio privatumo užtikrinantis. Įvesties trikdymas yra lengvai įgyvendinamas metodas, kuris leidžia sukurti išvalytą duomenų rinkinį, tinkamą taikyti skirtingiems ML algoritmams. Kadangi šis metodas orientuojasi į įvesties duomenų trikdymą ML modeliams, tą pačią procedūrą galima pritaikyti daugeliui skirtingų ML algoritmų. Pavyzdžiui, trikdyta kovariacijos matrica taip pat gali būti naudojama kaip įvestis daugeliui komponentų analizės algoritmų, tokių kaip pagrindinių komponentų analizė (PCA), linijinė diskriminantinė analizė (LDA) ir daugybinė diskriminantinė analizė (MDA). Be to, dauguma diferencinio privatumo mechanizmų gali būti taikomi įvesties trikdymo metoduose, priklausomai nuo įvesties duomenų savybių.

Naudojant algoritmo trikdymą (algorithm perturbation), privatūs duomenys yra perduodami ML algoritmui (galbūt po neprivataus duomenų išankstinio apdorojimo procedūrų), o tada taikomi diferencinio privatumo (DP) mechanizmai, siekiant sukurti atitinkamai išvalytus modelius. ML algoritmams, kuriems reikia kelių iteracijų ar etapų, diferencinio privatumo (DP) mechanizmai gali būti naudojami tarpiniams rezultatams (pvz., modelio parametrams) trikdyti kiekvienoje iteracijoje ar etape. Pavyzdžiui, pagrindinių komponentų analizei (PCA) atlikti gali būti naudojamas galios iteracijos metodas, kuris yra iteracinis algoritmas. Naudojant noisy power method, kiekvienoje algoritmo iteracijoje gali būti pridedamas Gauso triukšmas, kuris taikomas netrikdytai kovariacijos matricai (t. y., įvesčiai), taip sukuriant DP PCA. Palyginti su įvesties trikdymu, algoritmo trikdymas reikalauja specifinio dizaino, pritaikyto skirtingiems ML algoritmams. Tačiau jis paprastai įveda mažiau triukšmo, nes tarpinės reikšmės, gaunamos treniruojant ML modelius, paprastai turi mažesnį jautrumą nei pradiniai įvesties duomenys.

Naudojant išvesties trikdymą (output perturbation), pritaikomas neprivatus mokymosi algoritmas, o tada triukšmas pridedamas prie sukurto modelio. Apskritai išvesties trikdymo metodai dažniausiai taikomi ML algoritmams, kurie generuoja sudėtingą statistiką kaip savo ML modelius. Pavyzdžiui, požymių ištraukimo ir matmenų mažinimo algoritmai paprastai publikuoja išgautus požymius. Todėl naudojant projekcijos matricą matmenų mažinimui, išvesties trikdymas yra tinkamas scenarijus. Tačiau daugelis prižiūrimų ML algoritmų, kuriems reikia daug kartų publikuoti modelį ir sąveikauti su testavimo duomenimis, tokie kaip tiesinė regresija, logistinė regresija ir SVM, nėra tinkami išvesties trikdymui.

Tikslinės funkcijos trikdymas (objective perturbation) apima triukšmo pridėjimą prie tikslinės funkcijos tokiuose mokymosi algoritmuose kaip empirinės rizikos minimizavimas (empirical risk minimization). Pagrindinė empirinės rizikos minimizavimo idėja yra ta, kad negalime tiksliai žinoti, kaip gerai algoritmas veiks su realiais duomenimis, nes nežinome tikrojo duomenų skirstinio, su kuriuo algoritmas sąveikaus. Tačiau galime įvertinti jo našumą žinomame mokymo duomenų rinkinyje, o šį įvertinimą vadiname empirine rizika. Todėl tikslinės funkcijos trikdymo metu gali būti sukurtas vektorinės mechanizmas, leidžiantis pridėti triukšmą prie tikslinės funkcijos, užtikrinant diferencinį privatumą ir išlaikant algoritmo efektyvumą. https://jmlr.org/papers/volume12/chaudhuri11a/chaudhuri11a.pdf

* Differentially private naive Bayes classification (https://arxiv.org/abs/1905.01039)
* Differentially private logistic regression (https://systems.cs.columbia.edu/private-systems-class/papers/Chaudhuri2009Privacy.pdf)
* Differentially private linear regression (https://arxiv.org/abs/2007.05157)
* Differentially private naive Bayes classification (https://arxiv.org/abs/1905.01039)
* Differentially private logistic regression (https://systems.cs.columbia.edu/private-systems-class/papers/Chaudhuri2009Privacy.pdf)
* Differentially private linear regression (https://arxiv.org/abs/2007.05157)

Diferencinis privatumas (DP) yra plačiai pripažintas standartas individualaus privatumo kiekybiniam įvertinimui. Pagal pirminį DP apibrėžimą egzistuoja patikimas duomenų tvarkytojas, kuris renka duomenis iš asmenų ir taiko technikas, kad gautų diferencinio privatumo užtikrinančią statistiką. Tada šis duomenų tvarkytojas publikuoja privatumą saugančią statistiką apie populiaciją. Lokalaus diferencinio privatumo (LDP) atveju asmenys siunčia savo duomenis duomenų agregatoriui po to, kai duomenys yra privatizuojami trikdymo metodu. Šios technikos suteikia asmenims tikėtiną neigiamumą (plausible deniability). Duomenų agregatorius surenka visus trikdytus duomenis ir atlieka statistikos įvertinimą

* Direct encoding
* Histogram encoding
* Unary encoding
* Examples with code: https://programming-dp.com/ch13.html
* Survey (paper): https://onlinelibrary.wiley.com/doi/10.1155/2020/8829523

ML modelio našumas daugiausia priklauso nuo sukauptų mokymo duomenų kiekio ir kokybės. Kai neturima pakankamai duomenų mokymui, dažnai vyksta duomenų dalijimasis tarp organizacijų, turinčių panašius mokslinių tyrimų interesus. Tai leidžia išplėsti tyrimų mastą, tačiau privatumo problema išlieka ta pati. Duomenys paprastai apima jautrią asmeninę informaciją, kuri gali sukelti privatumo pažeidimus asmenims. Todėl būtina įdiegti privatumo apsaugos mechanizmus duomenų dalijimosi metu. Viena iš geriausių alternatyvų – generuoti privatumą saugančius sintetinius duomenis, kurie yra lankstus ir praktiškas sprendimas dalijantis jautriais duomenimis tarp kelių suinteresuotųjų šalių.

Sintetiniai duomenys yra dirbtinai suformuoti duomenys, kurie paprastai generuojami naudojant dirbtinius algoritmus, o ne renkami realaus pasaulio tiesioginio matavimo metodais. Tačiau jie vis tiek išlaiko tam tikras esmines faktines duomenų savybes, tokias kaip statistinės charakteristikos, funkcionalumas ar išvados.

Prieš išgaunant bet kokias statistines charakteristikas iš pradinio duomenų rinkinio, pirmasis žingsnis yra išankstinis apdorojimas, apimantis išskirčių pašalinimą ir požymių reikšmių normalizavimą. Išskirtys – tai duomenų taškai, kurie yra toli nuo kitų stebėjimų. Jos gali atsirasti dėl eksperimento rezultatų kintamumo ar matavimo klaidų, kurios kartais pateikia neteisingą informaciją duomenų naudotojams.
Daugeliu atvejų išskirtys gali suklaidinti sintetinį duomenų generatorių, priversdamos jį generuoti daugiau išskirčių, o tai daro ML modelį netikslų. Vienas iš įprastų išskirčių nustatymo būdų yra tankio metodas: stebėti, ar tikimybė tam tikroje srityje rasti tam tikrus taškus yra daug mažesnė už tikėtiną vertę toje srityje.
Kitas žingsnis – požymių normalizavimas. Kiekvienas duomenų rinkinys turi skirtingą požymių skaičių, o kiekvieno požymio reikšmių diapazonas gali būti skirtingas. Normalizavimas užtikrina, kad visi požymiai turėtų panašų skalės diapazoną, taip pagerindamas ML modelio stabilumą ir našumą. Normalizavus duomenų rinkinį, galima sukurti skirstinio ištraukimo modelį, kuris išlaiko originalių duomenų statistines charakteristikas.
Galiausiai atliekamas privatumo testas, siekiant užtikrinti, kad sugeneruoti sintetiniai duomenys atitinka tam tikras iš anksto nustatytas privatumo garantijas (pvz., k-anonimiškumą, diferencinį privatumą (DP) ir kt.). Jei sugeneruoti sintetiniai duomenys negali užtikrinti numatytų privatumo garantijų, privatumo testas bus nesėkmingas. Tokiu atveju sintetiniai duomenys generuojami pakartotinai, kol privatumo testas bus išlaikytas.
Dabar, kai aptarėme pagrindines sąvokas, taikymo scenarijus ir bendrą sintetinės duomenų generavimo procesą, pažvelkime į populiariausias sintetinės generacijos technikas, pagrįstas duomenų anonimizavimu ir diferenciniu privatumu. Pradėsime nuo duomenų anonimizavimo metodų. (Skaidrė 17)

Federated Learning, FL yra giliojo mokymosi sritis, kuri per pastaruosius dešimtmečius tapo galingu įrankiu įvairioms sudėtingoms problemoms spręsti. „Google“ pasiūlė federacinį mokymąsi kaip giliojo mokymosi variantą, siekdama spręsti duomenų savininkų privatumo problemas. Tradiciniai mašininio mokymosi (ML) algoritmai remiasi ekspertų sukurtais požymių inžinerijos metodais kuriant modelius.

Deep learning treniruoja tinklo modelius naudodamas didelius duomenų kiekius, kad modeliai galėtų atpažinti ir prognozuoti. Didėjant mokymo duomenų kiekiui, neuroninių tinklų tikslumas nuolat gerėja. Gilesni neuroninio tinklo sluoksniai gali visiškai išgauti požymius, todėl modeliai veikia efektyviau. Pastaraisiais metais, vystantis dideliems duomenų rinkiniams ir galingai aparatine įrangai, DL palaipsniui tapo pagrindiniu metodu apdoroti sudėtingus aukšto matmens duomenis, tokius kaip vaizdai, tekstai ir garso signalai.

* https://www.v7labs.com/blog/federated-learning-guide
* https://flower.ai/docs/framework/tutorial-series-what-is-federated-learning.html
* https://www.ibm.com/think/topics/deep-learning

Federated learning vulnerabilities. Kadangi klientams nereikia dalintis savo privačiais duomenimis su serveriu, pažeistas klientas gali tiesiogiai manipuliuoti vietinio modelio mokymo procesu, kad įvykdytų užnuodijimo atakas (poisoning attacks). Be to, kadangi klientai gali pasiekti bendrai treniruotą modelį per globalaus modelio transliavimą iš serverio, kenksmingas klientas gali pakartotinai sąveikauti su globaliu modeliu ir įgyvendinti išvedimo atakas (inference attacks). Atakos (pvz., išvedimo atakos) gali būti vykdomos serverio, pasinaudojant vietiniais atnaujinimais, siekiant atkurti klientų vietinius duomenis. Todėl serverio pažeidžiamumai turi būti nuolat stebimi, o pats serveris turi būti atsparus įvairioms atakoms. Federacinio mokymosi našumas labai priklauso nuo agregavimo proceso. Tradiciniai agregavimo algoritmai, tokie kaip Federated Averaging (FedAvg), yra pažeidžiami prieš priešiškas atakas. Saugūs agregavimo metodai yra būtini, siekiant užkirsti kelią smalsiam serveriui gauti prieigą prie klientų atnaujinimų ir efektyviai aptikti nenormalų klientų elgesį. Federacinis mokymasis reikalauja kelių komunikacijos etapų tarp dalyvių ir federacinio serverio. Nesaugūs komunikacijos kanalai kelia rimtą grėsmę. Pavyzdžiui, modelio atnaujinimai gali būti perimti arba pakeisti per „žmogus viduryje“ (man-in-the-middle) atakas. Be to, komunikacijos kliūtys gali padidinti klientų, pasitraukiančių iš federacinio mokymosi sistemų, skaičių.

Apsissaugojimas. Federaciniame mokymesi (FL) diferencinis privatumas (DP) taikomas siekiant apsaugoti dalyvių įkeltus parametrus. Esami DP pagrįsti metodai gali būti skirstomi į centralizuotą diferencinį privatumą (CDP), lokalų diferencinį privatumą (LDP) ir paskirstytą diferencinį privatumą (DDP). Tačiau šie metodai dažnai susiduria su kompromisu tarp mokymosi tikslumo ir privatumo apsaugos.
SMPC yra kriptografinis metodas, leidžiantis keliems dalyviams bendradarbiauti atliekant skaičiavimo užduotį ar vykdant matematinę funkciją, nesidalinant duomenimis tarpusavyje. SMPC taikymo metodai federacinio mokymosi sistemoje gali būti suskirstyti į dvi kategorijas:
* Serveriais pagrįstas metodas: Visi klientai siunčia apdorotus duomenų dalinius keliems serveriams, darant prielaidą, kad serveriai yra nepriklausomi ir ne visi yra pažeisti.
* Klientais pagrįstas metodas: Paprastai naudojamas tik vienas serveris, o didžioji dalis saugaus skaičiavimo pastangų tenka klientams.

Federaciniame mokymesi (FL) labai svarbu agreguoti klientų atnaujinimus užšifruotu būdu. Homomorfinis šifravimas (HE), pagrįstas matematiniais šifravimo metodais, leidžia atlikti operacijas su užšifruotais duomenimis ir sukuria šifruotą tekstą, kuriame yra užšifruotas rezultatas. Naudojant HE FL užtikrinama, kad modelio konvergencija nepatirs našumo praradimo [30]. Du populiarūs HE metodai, naudojami FL, yra Paillier ir ElGamal schemos.

Kitas požiūris orientuojasi į anoniminės komunikacijos ir maišymo modelių (shuffle models) naudojimą duomenų privatumo apsaugai. Anoniminės komunikacijos tikslas yra užtikrinti, kad informacijos gavėjas nežinotų pranešimo siuntėjo tapatybės. Bendras metodas apima anoniminio tinklo sukūrimą, kur užklausa praeina trijų žingsnių maršrutą, kad būtų paslėptas užklausos pateikėjo šaltinis.

Modelio inversijos atakos (Model Inversion Attacks, MIAs), dar vadinamos rekonstrukcijos atakomis (Reconstruction Attacks, RAs), siekia atkurti privačią informaciją iš viešų duomenų. Federaciniame mokymesi „privatūs duomenys“ RAs kontekste paprastai reiškia klientų vietinius duomenis, o „vieši duomenys“ – globalų modelį (prieinamą centriniam serveriui ir klientams) bei gradientus (prieinamus centriniam serveriui).

Savybių išvedimo atakos (Property Inference Attacks, PIAs) yra dar viena reikšminga grėsmė federaciniame mokymesi. Skirtingai nei rekonstrukcijos atakos, kurios orientuojasi į mokymo duomenų rinkinio duomenų reprezentacijos atskleidimą, PIAs siekia išgauti bendrą statistinę informaciją apie mokymo duomenų rinkinį. Pavyzdžiui, įsivaizduokite, kad įmonė naudoja savo vidinius el. laiškus šlamšto klasifikatoriui apmokyti. Rekonstrukcijos atakos atveju užpuolikai bandytų atskleisti konkrečius el. laiškus, siekdami gauti vidinių paslapčių. Tuo tarpu PIAs atveju užpuolikai gali būti suinteresuoti nustatyti bendrą el. laiškuose esančią nuotaikos pasiskirstymą, nes ši nuotaika gali atskleisti įžvalgų apie įmonės kultūrą ar finansinę padėtį.

Narystės išvedimo ataka (Membership Inference Attack, MIA) yra reikšminga grėsmė federaciniame mokymesi. MIA metu užpuolikai siekia nustatyti, ar tam tikri duomenų taškai (pvz., asmenys, vaizdai ir pan.) yra įtraukti į aukos modelių mokymo rinkinius. MIA kelia rimtą grėsmę federacinio mokymosi sistemoms. Teoriškai, kai federaciniai naudotojai yra institucijos, jie dažnai būna konkurenciniuose santykiuose. Pavyzdžiui, institucija A galėtų naudoti MIA, kad nustatytų, kurie jos duomenų bazės klientai taip pat yra institucijos B klientai, taip sukeldama nesąžiningą komercinę konkurenciją.

Federacinis mokymasis iš pradžių buvo sukurtas siekiant apsaugoti naudotojų privatumą, tačiau jo paskirstyta prigimtis daro jį pažeidžiamą užnuodijimo atakoms. Pagrindinis skirtumas tarp federacinio mokymosi ir tradicinio mašininio mokymosi yra mokymo procesas. Federaciniame mokymesi dalyviai atlieka dvi roles: serverio ir klientų. Serveris valdo globalaus modelio mokymą, o klientai įkelia savo atnaujinimus į serverį, prisidėdami prie globalaus modelio mokymo. Federacinio mokymosi struktūra neleidžia serveriui pasiekti klientų mokymo duomenų, o klientų duomenų rinkiniai paprastai nėra nepriklausomi ir vienodai paskirstyti. Tai daro federacinį mokymąsi idealiu taikiniu užnuodijimo atakoms. Įprastas taikinys yra globalus modelis, valdomas patikimo serverio. Priešininko tikslas yra panašus į tradicines užnuodijimo atakas: užkirsti kelią globalaus modelio konvergencijai arba įdiegti užpakalinę durų (backdoor) spragą. Federaciniame mokymesi priešininkai dažnai vykdo Bizantines atakas, kurios apima grėsmių modelį, kai kenkėjiški klientai siunčia savavališkus gradientų atnaujinimus serveriui, o nekenksmingi klientai siunčia švarius atnaujinimus. Tokiu būdu atakos procesas tampa strateginiu žaidimu tarp kenkėjiškų ir nekenksmingų klientų. Federacinio mokymosi užnuodijimo atakas galima suskirstyti į duomenų užnuodijimą ir modelio užnuodijimą. Duomenų užnuodijimo metu naudojamos technikos yra panašios į tradicinio giliojo mokymosi modelių mokymo metodus. Tačiau taikinys skiriasi: federaciniame mokymesi priešininkas gali tik įterpti ar modifikuoti kenkėjiškus duomenis pažeistų klientų duomenų rinkiniuose. Situacija skiriasi modelio užnuodijimo atakos atveju. Kadangi klientai serveriui yra kaip juodoji dėžė, kenkėjiška veikla gali likti nepastebėta.

Federaciniame mokymesi priešininkų tikslai yra panašūs į tuos, kurie yra tradiciniame mašininiame mokymesi: netikslinės, tikslinės ir „užpakalinių durų“ (backdoor) užnuodijimo atakos. Tarp jų „galinių durų“ užnuodijimo atakos yra pavojingiausios dėl jų slapto pobūdžio. 

Antrasis iššūkis yra atsakomųjų priemonių buvimas. Palyginti su centralizuotu mašininiu mokymusi, federacinis mokymasis turi tam tikrą atsparumo pranašumą. Federaciniame mokymesi agregavimo strategija yra pagrindinė, siekiant sukurti optimalų globalų modelį. Pavyzdžiui, FedAvg algoritmas atsitiktinai parenka tam tikrą klientų skaičių kiekvienam globalaus modelio mokymo etapui. Mokymo proceso paskirstyta prigimtis tarp skirtingų subjektų leidžia lengviau įgyvendinti ir integruoti atsakomąsias priemones. Dėl to atsakomosios priemonės gali būti sklandžiai įtrauktos į federacinio mokymosi struktūrą. Todėl vykdant užnuodijimo ataką federaciniame mokymesi, atsakomosios priemonės yra dar vienas iššūkis, kurį priešininkai turi įveikti.

Pagrindinis užnuodijimo atakų taikinys federaciniame mokymesi yra globalus modelis. Paprastai daroma prielaida, kad serveris yra patikimas, ir jo vaidmuo yra užtikrinti gynybos mechanizmų įgyvendinimą. Federaciniame mokymesi duomenys daugiausia perduodami tarp klientų ir serverio, o kenkėjiškų klientų užnuodyti duomenys seka tais pačiais keliais, kad pasiektų serverį. Todėl efektyviausias gynybos būdas nuo užnuodijimo atakos yra įdiegti apsaugos priemones tiesiai šiais duomenų perdavimo keliais.

Labiausiai akivaizdus užnuodijimo atakos poveikis yra jos įtaka globaliam modeliui. Vienas pagrindinių užnuodijimo atakos tikslų yra užkirsti kelią modelio konvergencijai. Daugelis užnuodytų atnaujinimų neigiamai veikia globalaus modelio vertinimo metrikas. Tačiau ne visi neigiami atnaujinimai federaciniame mokymesi yra kenkėjiški. Dauguma federacinio mokymosi treniravimo duomenų rinkinių nėra IID (nepriklausomai ir vienodai paskirstyti); todėl neigiamą poveikį modeliui gali lemti skirtingas individualių treniravimo duomenų rinkinių paskirstymas. Kai kuriais atvejais šis neigiamas poveikis netgi gali pagerinti modelio generalizacijos gebėjimą. Atskirti tikrai kenkėjiškus atnaujinimus nuo neigiamų atnaujinimų yra sudėtinga. Atnaujinimai yra aukšto matmens duomenys, o nėra visapusiško ar efektyvaus metodo, leidžiančio išmatuoti atstumą tarp aukšto matmens duomenų taškų. Esamos vertinimo metrikos, tokios kaip LpL_pLp -norma, gali įvertinti tik dalines dviejų aukšto matmens duomenų taškų savybes.
Todėl daugelis metodų nesiekia identifikuoti kenkėjiškų atnaujinimų, o sutelkia dėmesį į tai, kad globalaus modelio mokymas liktų nepažeistas net esant kenkėjiškiems atnaujinimams. Šios gynybos strategijos daugiausia orientuojasi į agregavimo procesą.

Duomenimis orientuoti metodai tampa žymiai mažiau veiksmingi, kai priešininkas kontroliuoja kelis kenkėjiškus klientus. Dauguma federacinio mokymosi sistemų klientų yra nekenksmingi. Siekdami padidinti atakos efektyvumą, kenkėjiški klientai dažnai padidina savo atnaujinamų duomenų reikšmes, dėl ko jie skiriasi nuo nekenksmingų duomenų. Tačiau Sybil atakų metu priešininkas kontroliuoja dalį klientų ir koordinuoja jų veiksmus. Užtikrindamas, kad kiekvieno kenkėjiško kliento atnaujinimai būtų labai panašūs į nekenksmingų klientų atnaujinimus, priešininkas gali sėkmingai įvykdyti ataką ir išvengti aptikimo.

Nors federacinis mokymasis (FL) turi aiškių privatumo pranašumų, jautri informacija FL sistemoje vis tiek gali būti išvesta naudojant išvedimo atakas, tokias kaip modelio inversijos atakos, modelio išvedimo atakos, savybių išvedimo atakos ir narystės išvedimo atakos.

Federaciniame mokymesi patikimas serveris gali matyti visų dalyvių duomenis ir atsakyti į užklausas privačiu būdu, atsitiktinai pakeisdamas užklausų rezultatus. Patikimas centrinis serveris atsakingas už triukšmo pridėjimą prie sujungtų vietinių modelių gradientų, kas padeda apsaugoti duomenų privatumo lygį pagal įrašus. Tačiau šios metodikos nepaiso to, kad centrinis serveris gali būti nepatikimas arba pažeistas priešininkų, kas gali sukelti privatumo nutekėjimą. Be to, CDP buvo sukurtas dirbti su pakankamai dideliu dalyvių skaičiumi, kad būtų pasiektas priimtinas kompromisas tarp privatumo ir tikslumo. Kai dalyvių skaičius yra mažas, FL modelis su CDP gali nesusikurti arba nepasiekti priimtino tikslumo.

Siekiant užpildyti tarpą tarp CDP ir LDP, buvo pasiūlytas paskirstytas diferencinis privatumą (Distributed Differential Privacy, DDP), kad būtų išvengta priklausomybės nuo patikimo centrinio serverio ir užtikrintas geresnis naudingumas. Šis metodas užtikrina, kad centrinis serveris gautų agregavimo rezultatus, o dalyvių parametrai nebus atskleisti serveriui. Norint papildomai užtikrinti, kad agreguotas rezultatas neatskleistų papildomos informacijos, LDP gali būti naudojamas trikdyti vietinius modelio parametrus prieš agregavimą. Kitas metodas DDP pasiekimui yra naudojant maišymo modelį (shuffle model).


In the practice of FL systems, DP is often not used directly and needs to be modified to meet the specific requirements of the task.

![image](https://github.com/user-attachments/assets/b922a54b-a94a-4215-9486-3a26f023a244)

